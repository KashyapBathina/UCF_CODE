{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f11107c-926f-4fa9-b41a-9509d3664ebb",
   "metadata": {},
   "source": [
    "## 1. Load and Preprocess the MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4dd265-9c38-4d2d-85f8-57a4497e5427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Comment: These lines load and split the MNIST dataset into training and test sets. \n",
    "# The `X_train` and `X_test` variables store the image data, while `y_train` and `y_test` hold the corresponding labels. \n",
    "# Printing the shape helps verify the dataset size and format for further processing.\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(X_train.shape)\n",
    "\n",
    "#Comment: This step normalizes the image data by converting pixel values to float32 and scaling them between 0 and 1. \n",
    "# Normalization helps improve model performance and stability during training by ensuring consistent input ranges.\n",
    "X_train = X_train.astype(\"float32\") / 255.0\n",
    "X_test = X_test.astype(\"float32\") / 255.0\n",
    "\n",
    "#Comment: Reshaping the dataset flattens each 28x28 image into a 1D array of 784 pixels. \n",
    "# This format is required as input for fully connected layers in the neural network.\n",
    "X_train = X_train.reshape(X_train.shape[0], 28*28)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28*28)\n",
    "\n",
    "\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Test data shape:\", X_test.shape)\n",
    "\n",
    "#Comment: This section visualizes a sample of the training images along with their corresponding labels. \n",
    "# Each image is reshaped back to 28x28, displayed in grayscale, and labeled for easy verification.\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(X_train[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f\"Label: {y_train[i]}\")\n",
    "    plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d62469-9b02-4496-b9d3-23dd5418c1a0",
   "metadata": {},
   "source": [
    "## Build the Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8db23b7-9d14-4159-85db-47f570d6b960",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "\n",
    "# Build the neural network model\n",
    "#Write you code here. Define \"model\" using Sequential, Input and Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "\n",
    "# Build the neural network model for Part 2\n",
    "model = Sequential([\n",
    "    Input(shape=(28*28,)),  # Input layer\n",
    "    Dense(8, activation='relu'),  # Hidden layer with 8 neurons\n",
    "    Dense(10, activation='softmax')  # Output layer with 10 classes\n",
    "])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a09151b-3eb9-4752-beda-512a294d78cd",
   "metadata": {},
   "source": [
    "## Train the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2d60ad-5621-460f-9a71-889f2dd04254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "#Comment: The `model.compile()` function configures the neural network for training. \n",
    "# The Adam optimizer is used for efficient gradient descent, the sparse categorical crossentropy loss function is chosen for multi-class classification, \n",
    "# and accuracy is selected as the metric to evaluate performance.\n",
    "model.compile(optimizer=Adam(),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Comment: The ModelCheckpoint callback saves the model's weights during training whenever the validation accuracy improves. \n",
    "# This ensures the best version of the model is saved for evaluation.\n",
    "checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_accuracy', mode='max')\n",
    "\n",
    "#Comment: The `model.fit()` function trains the neural network on the training dataset for 10 epochs with a batch size of 32. \n",
    "# It also validates the model on the test dataset after each epoch and uses the checkpoint to save the best-performing model.\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32, callbacks=[checkpoint])\n",
    "\n",
    "#Comment: These lines visualize training and validation accuracy over epochs. \n",
    "# They help monitor the model's learning progress and detect potential overfitting or underfitting.\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "#Comment: These lines visualize training and validation loss over epochs. \n",
    "# The graphs indicate how well the model is minimizing the loss function, which reflects prediction errors.\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68499a5-8cda-4dc0-b3a4-d54e048c20ba",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f37fc1-24c7-4ef6-b1b1-6af81e85f54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "best_model = tf.keras.models.load_model('best_model.keras')\n",
    "\n",
    "#Comment: The `evaluate()` method calculates the loss and accuracy of the saved best model on the test dataset. \n",
    "# This provides an unbiased evaluation of the model's generalization performance.\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "#Comment: This line predicts the probabilities of each class for all test samples. \n",
    "# These probabilities are then converted to class predictions using `np.argmax`, which selects the class with the highest probability.\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "\n",
    "#Comment: The confusion matrix compares true labels with predicted labels. \n",
    "# It provides insights into the model's performance, such as how well each class is predicted and where errors occur.\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "\n",
    "#Comment: The heatmap visualizes the confusion matrix, making it easier to interpret the performance of the model on each class. \n",
    "# Each cell shows the number of samples for a true vs. predicted label pair.\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues', xticklabels=[str(i) for i in range(10)], yticklabels=[str(i) for i in range(10)])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f3d982-e4ae-41cd-bd87-e0bf60c3bebd",
   "metadata": {},
   "source": [
    "## Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1e46c2-e0de-47b2-ad6f-5c672c857e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comment: This line randomly selects 10 samples from the test dataset. \n",
    "# These samples will be used to visualize the model's predictions.\n",
    "indices = np.random.choice(len(X_test), 10, replace=False)\n",
    "\n",
    "#Comment: This section displays the true labels and predicted labels for the selected samples. \n",
    "# It helps visualize individual predictions and identify any misclassifications.\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i, idx in enumerate(indices):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(X_test[idx].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f\"True: {y_test[idx]}\\nPred: {y_pred_classes[idx]}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#Comment: This line identifies the indices of all misclassified samples in the test dataset. \n",
    "# It enables further analysis of where the model struggled.\n",
    "incorrect_indices = np.where(y_test != y_pred_classes)[0]\n",
    "\n",
    "#Comment: This section visualizes the true labels and predicted labels for the first 10 misclassified samples. \n",
    "# These examples can help identify patterns in the model's errors.\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i, idx in enumerate(incorrect_indices[:10]):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(X_test[idx].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f\"True: {y_test[idx]}\\nPred: {y_pred_classes[idx]}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc519bd-ea03-43c7-8fa8-e1dec6636611",
   "metadata": {},
   "source": [
    "## Part 3: Experimenting with a Larger Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b572da81-f729-46cf-8b9e-ecdc30bdb4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building------------------------------------------------------------------------------------------------------\n",
    "# Build the neural network model for Part 3\n",
    "model_128 = Sequential([\n",
    "    Input(shape=(28*28,)),  # Input layer\n",
    "    Dense(128, activation='relu'),  # Hidden layer with 128 neurons\n",
    "    Dense(10, activation='softmax')  # Output layer\n",
    "])\n",
    "\n",
    "model_128.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training------------------------------------------------------------------------------------------------------\n",
    "# Compile the model\n",
    "model_128.compile(optimizer=Adam(),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Set up a checkpoint for the 128-neuron model\n",
    "checkpoint_128 = ModelCheckpoint('best_model_128.keras', save_best_only=True, monitor='val_accuracy', mode='max')\n",
    "\n",
    "# Train the model\n",
    "history_128 = model_128.fit(X_train, y_train,\n",
    "                            validation_data=(X_test, y_test),\n",
    "                            epochs=10,\n",
    "                            batch_size=32,\n",
    "                            callbacks=[checkpoint_128])\n",
    "\n",
    "# Plot training and validation accuracy and loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_128.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_128.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy for 128 Neurons')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_128.history['loss'], label='Training Loss')\n",
    "plt.plot(history_128.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss for 128 Neurons')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Evaluation------------------------------------------------------------------------------------------------------\n",
    "# Load the best model for 128 neurons\n",
    "best_model_128 = tf.keras.models.load_model('best_model_128.keras')\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss_128, test_accuracy_128 = best_model_128.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy for 128-Neuron Model: {test_accuracy_128:.4f}\")\n",
    "\n",
    "# Predict class probabilities and convert to class labels\n",
    "y_pred_128 = best_model_128.predict(X_test)\n",
    "y_pred_classes_128 = np.argmax(y_pred_128, axis=1)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred_classes_128))\n",
    "\n",
    "# Generate and display the confusion matrix\n",
    "cm_128 = confusion_matrix(y_test, y_pred_classes_128)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_128, annot=True, fmt=\"d\", cmap='Blues', \n",
    "            xticklabels=[str(i) for i in range(10)], \n",
    "            yticklabels=[str(i) for i in range(10)])\n",
    "plt.title('Confusion Matrix for 128 Neurons')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Visualization------------------------------------------------------------------------------------------------------\n",
    "# Randomly select some test samples and visualize predictions\n",
    "indices_128 = np.random.choice(len(X_test), 10, replace=False)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i, idx in enumerate(indices_128):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(X_test[idx].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f\"True: {y_test[idx]}\\nPred: {y_pred_classes_128[idx]}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize incorrectly classified samples\n",
    "incorrect_indices_128 = np.where(y_test != y_pred_classes_128)[0]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i, idx in enumerate(incorrect_indices_128[:10]):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(X_test[idx].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f\"True: {y_test[idx]}\\nPred: {y_pred_classes_128[idx]}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fd4f77-dfc1-47b0-9861-3642045a687d",
   "metadata": {},
   "source": [
    "## Part 4: Custom Neural Network For 99% Average F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa74600-c7ed-432b-962d-63c012b5820b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Building------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Define the custom neural network model for Part 4\n",
    "model_custom_4 = Sequential([\n",
    "    Input(shape=(28*28,)),  # Input layer\n",
    "    Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),  # First hidden layer with L2 regularization\n",
    "    Dropout(0.4),  # Dropout for regularization\n",
    "    Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),  # Second hidden layer\n",
    "    Dropout(0.4),  # Dropout for regularization\n",
    "    Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),  # Third hidden layer\n",
    "    Dropout(0.3),  # Dropout for regularization\n",
    "    Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),  # Fourth hidden layer\n",
    "    Dense(10, activation='softmax')  # Output layer\n",
    "])\n",
    "\n",
    "model_custom_4.summary()\n",
    "\n",
    "# Training------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Compile the custom model\n",
    "model_custom_4.compile(optimizer=Adam(),\n",
    "                       loss='sparse_categorical_crossentropy',\n",
    "                       metrics=['accuracy'])\n",
    "\n",
    "# Set up a checkpoint for the custom model\n",
    "checkpoint_custom_4 = ModelCheckpoint('best_model_custom_4.keras', save_best_only=True, monitor='val_accuracy', mode='max')\n",
    "\n",
    "# Train the model\n",
    "history_custom_4 = model_custom_4.fit(X_train, y_train,\n",
    "                                      validation_data=(X_test, y_test),\n",
    "                                      epochs=15,  # Increased epochs for better convergence\n",
    "                                      batch_size=64,  # Adjusted batch size for performance\n",
    "                                      callbacks=[checkpoint_custom_4])\n",
    "\n",
    "# Plot training and validation accuracy and loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_custom_4.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_custom_4.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy for Part 4')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_custom_4.history['loss'], label='Training Loss')\n",
    "plt.plot(history_custom_4.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss for Part 4')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Evaluating------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Load the best custom model\n",
    "best_model_custom_4 = tf.keras.models.load_model('best_model_custom_4.keras')\n",
    "\n",
    "# Evaluate the custom model\n",
    "test_loss_custom_4, test_accuracy_custom_4 = best_model_custom_4.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy for Part 4: {test_accuracy_custom_4:.4f}\")\n",
    "\n",
    "# Predict class probabilities and convert to class labels\n",
    "y_pred_custom_4 = best_model_custom_4.predict(X_test)\n",
    "y_pred_classes_custom_4 = np.argmax(y_pred_custom_4, axis=1)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred_classes_custom_4))\n",
    "\n",
    "# Generate and display the confusion matrix\n",
    "cm_custom_4 = confusion_matrix(y_test, y_pred_classes_custom_4)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_custom_4, annot=True, fmt=\"d\", cmap='Blues',\n",
    "            xticklabels=[str(i) for i in range(10)],\n",
    "            yticklabels=[str(i) for i in range(10)])\n",
    "plt.title('Confusion Matrix for Part 4')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Visualizing------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Randomly select some test samples and visualize predictions\n",
    "indices_custom_4 = np.random.choice(len(X_test), 10, replace=False)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i, idx in enumerate(indices_custom_4):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(X_test[idx].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f\"True: {y_test[idx]}\\nPred: {y_pred_classes_custom_4[idx]}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize incorrectly classified samples\n",
    "incorrect_indices_custom_4 = np.where(y_test != y_pred_classes_custom_4)[0]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i, idx in enumerate(incorrect_indices_custom_4[:10]):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(X_test[idx].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f\"True: {y_test[idx]}\\nPred: {y_pred_classes_custom_4[idx]}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
